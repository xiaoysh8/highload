How to delete a million files

On the servers can accumulate mountains of files, which periodically need to be deleted. For example, logs, compiled versions of files, or any other file cache generated by scripts.

Sooner or later these mountains have to be cleaned:

$ rm /tmp/logs/*.log
If the number of files is critically large, at some point, instead of deleting files, we'll see such a message in the console:

/bin/rm: Argument list too long.
What does this mean?

Problem

The fact is that the use of a mask in commands like rm / cp / find linux translates into a convenient format for itself, from a human-readable command:

$ rm /tmp/logs/*.log
list of files under this mask:

$ rm /tmp/logs/1.log /tmp/logs/2.log /tmp/logs/3.log ...
Problems begin when the rm command arguments become larger than the allowable limit. You can check this limit with the getconf command:

$ getconf ARG_MAX
262144
And what should I do?

Use the For loop

The easiest way is to execute the command we need in the for loop, which has two important advantages. First, the cycles are non-resource-intensive and do not have limits on the number of arguments. Secondly, in a loop simply wrap additional logic, if you need to do something more complicated than deleting files.

For example, here's how you can delete all files with one command:

$ for f in /tmp/logs/*.log; do rm "$f"; done
Or delete files that are older than seven days:

for f in /tmp/logs/*.log
do
  find $f -mtime +7 -exec rm {} \;
done
Or to count, write down in a variable and deduce their quantity:

FILES_COUNT=`c=0; for f in /tmp/logs/*.log ; do ((c++)); done ; echo $c`
echo "$FILES_COUNT log files left";
The most important thing

Do not forget to clean logs regularly so as not to clog the file system
Use for this purpose ready-made tools so as not to invent your bicycles
Reread commands before execution, so that you do not accidentally delete everything